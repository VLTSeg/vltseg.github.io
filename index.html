<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="By simply fine-tuning a strong vision-language pre-trained backbone without any additional modules or methods, our approach shows similar or stronger performance than previous works for segmentation and detection.">
  <meta property="og:title" content="Strong but Simple: A Baseline for Domain Generalized Dense Perception"/>
  <meta property="og:description" content="By simply fine-tuning a strong vision-language pre-trained backbone without any additional modules or methods, our approach shows similar or stronger performance than previous works for segmentation and detection."/>
  <meta property="og:url" content="https://vltseg.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/socials_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="879"/>


  <meta name="twitter:title" content="Strong but Simple: A Baseline for Domain Generalized Dense Perception">
  <meta name="twitter:description" content="By simply fine-tuning a strong vision-language pre-trained backbone without any additional modules or methods, our approach shows similar or stronger performance than previous works for segmentation and detection.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/socials_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Domain Generalization, Semantic Segmentation, Object Detection, VLTSeg, VLTDet">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Strong but Simple: A Baseline for Domain Generalized Dense Perception</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
<!-- Page header -->  
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Strong but Simple: A Baseline for Domain Generalized Dense Perception by CLIP-based Transfer Learning</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="https://scholar.google.de/citations?user=VnihmRkAAAAJ&hl=en" target="_blank">Christoph Huemmer</a><sup>1,3*</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.de/citations?user=eqsXwGIAAAAJ&hl=en" target="_blank">Manuel Schwonberg</a><sup>1,3*</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=VtIjfmgAAAAJ&hl=en" target="_blank">Liangwei Zhou</a><sup>2*</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=O7qS9DkAAAAJ&hl=en" target="_blank">Hu Cao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.de/citations?user=-CA8QgwAAAAJ&hl=en" target="_blank">Alois Knoll</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.de/citations?user=5cOHZ4MAAAAJ&hl=en" target="_blank">Hanno Gottschalk</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"> <sup>1</sup>Technical University Berlin, <sup>2</sup>Technical University Munich, <sup>3</sup>CARIAD SE<br>ACCV 2024</span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2312.02021.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

                   
                    
              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/VLTSeg/VLTSeg" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Unofficial Reimplementation)</span>
                </a>
              </span>

              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.02021.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End page header -->

<!-- Model teaser-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body is-centered has-text-centered">
      <img src="static/images/Element12.png" style='height: 70%; width: 70%' />
      <h2 class="subtitle has-text-centered">
        Strong but simple: We simply transfer CLIP-based pre-trained weights (1), conduct fine-tuning on two different tasks
        (2) and evaluate the performance across various unseen real-world domains (3)
      </h2>
    </div>
  </div>
</section>
<!-- End model teaser -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Domain generalization (DG) remains a significant challenge for perception based on deep neural networks (DNNs), where domain shifts occur due to synthetic data, lighting, weather, or location changes.
            Vision-language models (VLMs) marked a large step for the generalization capabilities and have been already applied to various tasks.
            Very recently, first approaches utilized VLMs for domain generalized segmentation and object detection and obtained strong generalization.
            However, all these approaches rely on complex modules, feature augmentation frameworks or additional models.
          </p>
          <p>
            Surprisingly and in contrast to that, we found that simple fine-tuning of vision-language pre-trained models yields competitive or even stronger generalization results while being extremely simple to apply.
            Moreover, we found that vision-language pre-training consistently provides better generalization than the previous standard of vision-only pre-training.
            This challenges the standard of using ImageNet-based transfer learning for domain generalization.
          </p>
          <p>
            Fully fine-tuning a vision-language pre-trained model is capable of reaching the domain generalization SOTA when training on the synthetic GTA5 dataset.
            Moreover, we confirm this observation for object detection on a novel synthetic-to-real benchmark.
            We further obtain superior generalization capabilities by reaching 77.9% mIoU on the popular Cityscapes→ACDC benchmark.
            We also found improved in-domain generalization, leading to an improved SOTA of 86.4% mIoU on the Cityscapes test set, marking the first place on the leaderboard.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image section -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2"> Visual Results</h2>
        <h2 class="title is-5"> Cityscapes→ACDC {EVA-02-CLIP Encoder}</h2>
        <img src="static/images/0822-supp_cropped_lower.png" alt="MY ALT TEXT"/>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2"> Comparison of Fine-Tuning for DGSS</h2>
        <img src="static/images/FT_pre-trainings.png" alt="MY ALT TEXT"/>
        <div class="content">
          <p>
            Transfer Learning: Domain Generalization Performance of supervised
            training on GTA5 and SYNTHIA. * denotes that the ViT-L-16 was used as the encoder. ◦
            denotes that ViT-L-14 was used as the encoder. &#x1f512 denotes frozen encoder weights.
          </p>
        </div>
        <img src="static/images/SOTA_Table.png" alt="MY ALT TEXT"/>
        <div class="content">
          <p>
            State-of-the-Art performance of VLTSeg on Cityscapes and ACDC Test Set.
          </p>
        </div>
        <img src="static/images/finetuning_ablation.png" alt="MY ALT TEXT"/>
        <div class="content">
          <p>
            Ablation of freezing encoder layers for the syn-to-real DG performance. Training on GTA5 
            with EVA-02-ViT-B-16. Dashed lines indicate full fine-tuning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image section -->

<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-centered has-text-centered">
      <h2 class="title">Poster</h2>

      <iframe src="static/pdfs/ACCV2024_poster-final.pdf" width="100%" height="550"></iframe>
    </div>
  </div>
</section>
<!--End paper poster -->


<!--BibTex citation -->
<section class="section hero" id="BibTeX">
  <div class="container is-max-desktop content has-text-centered">
    <h2 class="title">BibTeX</h2>
    <div class="has-text-justified">
      <pre><code>@inproceedings{huemmer2024accv,
    author={Hümmer, Christoph and Schwonberg, Manuel and Zhou, Liangwei and Cao, Hu and Knoll, Alois and Gottschalk, Hanno},
    booktitle={ACCV},
    title={Strong but Simple: A Baseline for Domain Generalized Dense Perception by CLIP-based Transfer Learning},
    year={2024}
}</code></pre>
    </div>
  </div>
</section>
<!--End BibTex citation -->

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was published with <a href="https://pages.github.com" target="_blank">GitHub Pages</a>.
          </p>
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>
          <p>
            <a href="impressum.html#main" target="_blank">Impressum</a> / <a href="legal-notice.html#main" target="_blank">Legal Notice</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
<!-- End footer -->

<!-- Statcounter tracking code -->
  
  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->
</body>
</html>
